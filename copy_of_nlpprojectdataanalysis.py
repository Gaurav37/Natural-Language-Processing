# -*- coding: utf-8 -*-
"""Copy of NLPProjectDataAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1od9rEA70TmmD90nOxHiE_Abor3ufiTf_

Analysis
In the SQuAD2.0 dataset, we have tried to explore 75 percent of the training data. We wanted to come up with the
answer to “type” of questions, answers, and context alongside numbers. Upon further analysis, we came up with concepts of LDA (Latent Dirichlet Allocation) but LDA required training which would mean we need to have labeled
data. We have unlabeled data and now this could be a project
in itself to cluster unlabeled datasets. We also came across
different methods to deal with the problem of unlabeled data,
some of them were BERT-based models that could deal with
semi unlabeled data but going that way would mean deviation from our current project title. Just for data exploration, we required something pretrained and generalized
enough in a way that could be just like having an optimal
N number of clusters of unlabeled data. Spacy name entity
recognition seemed a perfect choice for this task as it’s pretrained and seems sophisticated enough for the task of data
exploration. We used Spacy’s “en core web sm” library as
its smaller than other two available and yet has similar F1
rates. It has 18 types of data categories namely ‘PERSON’,
’NORP’, ’FAC’, ’ORG’, ’GPE’, ’LOC’, ’PRODUCT’,
’EVENT’, ’WORK OF ART’, ’LAW’, ’LANGUAGE’,
’DATE’, ’TIME’, ’PERCENT’, ’MONEY’, ’QUANTITY’,
’ORDINAL’, ’CARDINAL’. For the task of type of question asked, we passed context, title, and combined questionanswer to Spacy NER algorithm. We found that Spacy
doesn’t work with single entities i.e. if we pass just a person’s name it returns None, as it is based upon the grammar
of the language and just one word could mean a noun, adjective, or anything. We then generated title-labels from the
context labels if the context contained the title within. That’s
how we came up with title labels. Then we passed combined
question and answers hoping that this way it can return much
better labels for question-answer and we were successful.
Next, we tried to find the number of each label type in titles
and for each of those titles, the number of questions type.
We found from the following plot that the majority of titles
were of three types i.e. Person, organization, GPE (countries, states, cities).
After this, we tried to plot the question types for all these
categories of title types and came up with the following
plot. It shows that most of the questions for each label type
ask about persons, organizations, places, dates, and numbers which do not fall under other categories. Some relationships seen were: Norp(Nationality, religious groups) links
with GPE(places, countries, cities, etc.) and dates. Organizations link with Person, organization, places, dates, ranks,
and other kinds of numbers.
After this, we tried to analyze our data from the length
of context, questions and answers perspective and we found
the following figures suggesting that most answers are of
length 6, while most contexts are of length 500-600 characters and most questions have a length of 110 approximately.
One point to be noted is that we tried to normalize this data
by choosing distinct values of all contexts, questions, and
answers.
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/

# Commented out IPython magic to ensure Python compatibility.
# %pwd

!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json
!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json

# Commented out IPython magic to ensure Python compatibility.
#%rm dev-v2.0.json
# %ls

import pandas as pd
df=pd.read_json(r'train-v2.0.json')
#df2=df.to_csv(r'train-v2.0.json')
#csvDf = df.to_csv(index=True)
#print(csvDf)

#topic=df.iloc[0,0]
#print(topic)
#df.drop(['version'], axis=1)
df_train=df['data']
#print(df['v'])
print(df_train)

!pip install --upgrade spacy==2.2.0 allennlp==0.9.0
import spacy
print(spacy.__version__)

def formatter(df):

  context=[]
  questions=[]
  answers_text=[]
  answers_start=[]
  title=[]
  for i in range(0,442):
    topic=df[i]['paragraphs']
    title_=df[i]['title']
    for sub_para in topic:
      for q_a in sub_para['qas']:
        questions.append(q_a['question'])
        if len(q_a['answers'])>0:
          answers_start.append(q_a['answers'][0]['answer_start'])
          answers_text.append(q_a['answers'][0]['text'])
        else:
          answers_start.append(None)
          answers_text.append(None)
        context.append(sub_para['context'])
        title.append(title_)
  return context,questions, answers_text, answers_start,title

# rows = zip(formatter(df['data']))

#Commenting out as of now as we might not convert it to csv format 
#rows = zip(formatter(df_train))
# import csv
# with open("train.csv", "w") as f:
#     writer = csv.writer(f)
#     for row in rows:
#         writer.writerow(row)

context,questions,answers_text,answers_start,title=formatter(df_train)

#Remove punctuations from tokens if any
#However, Spacy removes punctuations automatically as seen in testing cell
def rem_punct(sentence):
  punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~'''
  no_punct=""
  for char in sentence:
    if char in punctuations:
        no_punct = no_punct + " "
    else:
      no_punct =no_punct + char
  return no_punct

title_nopunc=[]
for i in range(0, len(title)):
  title_nopunc.append(rem_punct(title[i]))
print(title_nopunc)

## NER NEXT

nlp = spacy.load("en_core_web_sm")
def ner(topic):
  
  doc = nlp(topic)
  entities = []
  labels = []
  # position_start = []
  # position_end = []

  for ent in doc.ents:
    entities.append(ent)
    labels.append(ent.label_)
    # position_start.append(ent.start_char)
    # position_end.append(ent.end_char)
    #ner = pd.DataFrame({'Entities':entities,'Labels':labels})#,'Position_Start':position_start, 'Position_End':position_end})
  return entities, labels

#calling ner method for every value in context list
import math

ent_context=[]
lab_context=[]
# ent_context2=[]
# lab_context2=[]
limit=math.floor(len(context)*0.75)
print(limit)
for i in range(0,limit):#,len(context)):
  c1,c2=ner(context[i])
  ent_context.append(c1)
  lab_context.append(c2)
  if i%20==0:
    print(i)

#Wait till number becomes 97728
# CREATING TOPIC_LABELS ie lab_title
lab_title=[]
counter=0
for i in range(0,len(ent_context)):
  if len(ent_context[i])<1:
    lab_title.append(title_nopunc[i])
  else:
    for j in range(0,len(ent_context[i])):
      if title_nopunc[i] == str(ent_context[i][j]):
        lab_title.append(str(lab_context[i][j]))
        break
      else:
        if (j==len(ent_context[i])-1):
          lab_title.append(title_nopunc[i])
        else:
          counter+=1
# here we are putting title values in lab_title if either spacy context has no value

#Lets join question and answers so that in cases where spacy doesn't recognize 
print(answers_text[1])
answers_text, questions
ques_ans=[]
for i in range(0,len(questions)):
  ques_ans.append(str(questions[i])+ " " + str(answers_text[i]))
# abc=['abc','def','ghi']
# de=['ijk','lmn','opq']
# ef=[]
# for i in range(0,len(abc)):
#   ef.append(abc[i] + " " + de[i])
# print(ef[0])

# print(answers_text[0])
# for i in range(0,10):
#   print(ques_ans[i])

#Now lets pass the questions and answers_text combo to ner

ent_ques_ans=[]
lab_ques_ans=[]
# ent_context2=[]
# lab_context2=[]
limit=math.floor(len(ques_ans)*0.75)
print(limit)
for i in range(0,limit):#,len(context)):
  c1,c2=ner(ques_ans[i])
  ent_ques_ans.append(c1)
  lab_ques_ans.append(c2)
  if i%4==0:
    print(i)

print(ent_ques_ans[8176])

#We have title_labels. Now is time for labeling questions
#print(stringent_context[0]).index(title[0]))
# print(ent_context[8177])
# print(lab_context[8175])
print(len(ent_context))
print(len(lab_title))
print(counter)
print(title_nopunc.index("Prime minister"))
print(context[8176])


# print(type(title[0]))
# print(len(str(ent_context[0][8])))

# print("Hi")
# for i in range(0, len(ent_context[8176])):
#   print("I am here")
#   if str(ent_context[8176][i])==title[8176]:
#     print("yes")
#   else:
#     print(ent_context[8176][i],"title", title[8176])

#df2['ent_context'].isnull().sum()
#print(df2.loc[[8176]])
#df2.loc[df2['ent_context'] == ' ']

print(title_nopunc[0])
print(lab_title[0])
print(ent_context[0])
print(lab_context[0])
print(ent_ques_ans[0])
print(lab_ques_ans[0])
print(len(ent_ques_ans))

null_ent_context_index=[]
null_ent_ques_ans_index=[]
for i in range(0,97739):
  if not ent_ques_ans[i]:
    null_ent_ques_ans_index.append(i)
  if not ent_context[i]:
    null_ent_context_index.append(i)
print(null_ent_context_index)
print(null_ent_ques_ans_index)

#print(question[6])
print(answers_text[6])
print(context[6])
print(ent_context[6][9])
print(lab_context[6][9])
print(title_nopunc[6])
print(lab_title[6])
print(lab_ques_ans[0][0])

#METHOD 1 FOR PLOTS - PLOTTING AMONGST LABELS- TOPIC LABELS VS QUES_ANS LABELS

labels=['PERSON','NORP','FAC','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW','LANGUAGE','DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']
# num_of_lab_title=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
# num_of_lab_quesans=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
num_Person_ques=[]
num_Norp_ques=[]
num_Fac_ques=[]
num_Org_ques=[]
num_Gpe_ques=[]
num_Loc_ques=[]
num_Product_ques=[]
num_Event_ques=[]
num_Workofart_ques=[]
num_Law_ques=[]
num_Language_ques=[]
num_Date_ques=[]
num_Time_ques=[]
num_Percent_ques=[]
num_Money_ques=[]
num_Quantity_ques=[]
num_Ordinal_ques=[]
num_Cardinal_ques=[]
count=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]# Number of topics of each label category
for i in range(0,97739):
  if lab_title[i]=='PERSON':
    num_Person_ques.append(lab_ques_ans[i])
    count[0]+=1
  elif lab_title[i]=='NORP':
    num_Norp_ques.append(lab_ques_ans[i])
    count[1]+=1
  elif lab_title[i]=='FAC':
    num_Fac_ques.append(lab_ques_ans[i])
    count[2]+=1
  elif lab_title[i]=='ORG':
    num_Org_ques.append(lab_ques_ans[i])
    count[3]+=1
  elif lab_title[i]=='GPE':
    num_Gpe_ques.append(lab_ques_ans[i])
    count[4]+=1
  elif lab_title[i]=='LOC':
    num_Loc_ques.append(lab_ques_ans[i])
    count[5]+=1
  elif lab_title[i]=='PRODUCT':
    num_Product_ques.append(lab_ques_ans[i])
    count[6]+=1
  elif lab_title[i]=='EVENT':
    num_Event_ques.append(lab_ques_ans[i])
    count[7]+=1
  elif lab_title[i]=='WORK_OF_ART':
    num_Workofart_ques.append(lab_ques_ans[i])
    count[8]+=1
  elif lab_title[i]=='LAW':
    num_Law_ques.append(lab_ques_ans[i])
    count[9]+=1
  elif lab_title[i]=='LANGUAGE':
    num_Ques_ques.append(lab_ques_ans[i])
    count[10]+=1
  elif lab_title[i]=='DATE':
    num_Date_ques.append(lab_ques_ans[i])
    count[11]+=1
  elif lab_title[i]=='TIME':
    num_Time_ques.append(lab_ques_ans[i])
    count[12]+=1
  elif lab_title[i]=='PERCENT':
    num_Percent_ques.append(lab_ques_ans[i])
    count[13]+=1
  elif lab_title[i]=='MONEY':
    num_Money_ques.append(lab_ques_ans[i])
    count[14]+=1
  elif lab_title[i]=='QUANTITY':
    num_Quantity_ques.append(lab_ques_ans[i])
    count[15]+=1
  elif lab_title[i]=='ORDINAL':
    num_Ordinal_ques.append(lab_ques_ans[i])
    count[16]+=1
  elif lab_title[i]=='CARDINAL':
    num_Cardinal_ques.append(lab_ques_ans[i])
    count[17]+=1
  else:
    None
#num_of_topics_per_label=[]
num_of_topics_per_label=count.copy()
###########################METHOD 2: for each topic_name calculate number of QUES_ANS_LABELS
# number_lab_ques=[]
# for i in range(0,97739):
#   matr1=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
#   for j in(0,len(lab_ques_ans[i])):
#     for k in range(0,len(labels)):
#       print(i,j)
#       print(lab_ques_ans[i][j])
#       if lab_ques_ans[i][j]==labels[k]:
#         matr1[k]+=1
#   number_lab_ques.append(matr1)
# print(number_lab_ques)

def augment(ar):
  #ar = [['a','b','c'],['d','e','f']]
  concat_list = [j for i in ar for j in i]
  return concat_list
num_Norp_ques=	   augment(num_Norp_ques)
num_Person_ques=   augment(num_Person_ques)
num_Fac_ques=      augment(num_Fac_ques)
num_Org_ques=      augment(num_Org_ques)
num_Gpe_ques=      augment(num_Gpe_ques)
num_Loc_ques=      augment(num_Loc_ques)
num_Product_ques=  augment(num_Product_ques)
num_Event_ques=    augment(num_Event_ques)
num_Workofart_ques=augment(num_Workofart_ques)
num_Law_ques=      augment(num_Law_ques)
num_Language_ques= augment(num_Language_ques)
num_Date_ques=     augment(num_Date_ques)
num_Time_ques=     augment(num_Time_ques)
num_Percent_ques=  augment(num_Percent_ques)
num_Money_ques=    augment(num_Money_ques)
num_Quantity_ques= augment(num_Quantity_ques)
num_Ordinal_ques=  augment(num_Ordinal_ques)
num_Cardinal_ques= augment(num_Cardinal_ques)

def count(ar):
  counter=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
  for i in range(0,len(ar)):
    # if not ent_ques_ans[i]:
    #   c0+=1
    # counter=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]# Number of topics of each label category

    if ar[i]=='PERSON':
      counter[0]+=1
    elif ar[i]=='NORP':
      counter[1]+=1
    elif ar[i]=='FAC':
      counter[2]+=1
    elif ar[i]=='ORG':
      counter[3]+=1
    elif ar[i]=='GPE':
      counter[4]+=1
    elif ar[i]=='LOC':
      counter[5]+=1
    elif ar[i]=='PRODUCT':
      counter[6]+=1
    elif ar[i]=='EVENT':
      counter[7]+=1
    elif ar[i]=='WORK_OF_ART':
      counter[8]+=1
    elif ar[i]=='LAW':
      counter[9]+=1
    elif ar[i]=='LANGUAGE':
      counter[10]+=1
    elif ar[i]=='DATE':
      counter[11]+=1
    elif ar[i]=='TIME':
      counter[12]+=1
    elif ar[i]=='PERCENT':
      counter[13]+=1
    elif ar[i]=='MONEY':
      counter[14]+=1
    elif ar[i]=='QUANTITY':
      counter[15]+=1
    elif ar[i]=='ORDINAL':
      counter[16]+=1
    elif ar[i]=='CARDINAL':
      counter[17]+=1
    else:
      None
  return counter
num_Person_ques   =count(num_Person_ques   )
num_Norp_ques	  =count(num_Norp_ques		)
num_Fac_ques      =count(num_Fac_ques      )
num_Org_ques      =count(num_Org_ques      )
num_Gpe_ques      =count(num_Gpe_ques      )
num_Loc_ques      =count(num_Loc_ques      )
num_Product_ques  =count(num_Product_ques )
num_Event_ques    =count(num_Event_ques    )
num_Workofart_ques=count(num_Workofart_ques)
num_Law_ques      =count(num_Law_ques      )
num_Language_ques =count(num_Language_ques )
num_Date_ques     =count(num_Date_ques     )
num_Time_ques     =count(num_Time_ques     )
num_Percent_ques  =count(num_Percent_ques  )
num_Money_ques    =count(num_Money_ques    )
num_Quantity_ques =count(num_Quantity_ques )
num_Ordinal_ques  =count(num_Ordinal_ques  )
num_Cardinal_ques =count(num_Cardinal_ques )

print(len(num_Loc_ques))#number of questions type for title type=NORP
print(len(num_of_topics_per_label))# Number of topics in each(18) label category

df2 = pd.DataFrame(list(zip(title_nopunc, lab_title,ent_context,lab_context,ent_ques_ans,lab_ques_ans)),columns =['title_nopunc', 'lab_title','ent_context','lab_context','ent_ques_ans','lab_ques_ans']) 
df2

df2.loc[(df2.title_nopunc == 'Beyoncé'),'title_nopunc']='Hip-Hop'
df

# After discovering topics and contexts as entity we need to find NER for questions and then
# Plot those entity types of questions for each entity type of Topics
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

sns.set_style("darkgrid")
#plt.plot(np.cumsum(np.random.randn(1000,1)))
plt.figure(figsize=(16, 8),facecolor="white")
plt.plot(labels,num_of_topics_per_label)
plt.xlabel("Types of Label types")
plt.xticks(rotation=45)
plt.ylabel("Number of Titles for each label type")
plt.title("Number of title Vs title-type")
plt.show()

#Below plot for highest 3 label types in title  
sns.set_style("darkgrid")
#plt.plot(np.cumsum(np.random.randn(1000,1)))
plt.figure(figsize=(16, 8),facecolor="white")
plt.plot(labels,num_Person_ques   ,label="Person")
plt.plot(labels,num_Norp_ques	  ,label="Norp")#num_Person_ques
plt.plot(labels,num_Fac_ques      ,label="FAC")
plt.plot(labels,num_Org_ques      ,label="ORG")
plt.plot(labels,num_Gpe_ques      ,label="GPE")
plt.plot(labels,num_Loc_ques      ,label="LOC")
plt.plot(labels,num_Product_ques  ,label="PRODUCT")
plt.plot(labels,num_Event_ques    ,label="EVENT")
plt.plot(labels,num_Workofart_ques,label="WORK_OF_ART")
plt.plot(labels,num_Law_ques      ,label="LAW")
plt.plot(labels,num_Language_ques ,label="LANGUAGE")
plt.plot(labels,num_Date_ques     ,label="DATE")
plt.plot(labels,num_Time_ques     ,label="TIME")
plt.plot(labels,num_Percent_ques  ,label="PERCENT")
plt.plot(labels,num_Money_ques    ,label="MONEY")
plt.plot(labels,num_Quantity_ques ,label="QUANTITY")
plt.plot(labels,num_Ordinal_ques  ,label="ORDINAL")
plt.plot(labels,num_Cardinal_ques ,label="CARDINAL")

plt.xlabel("Types of Question Label types")
plt.xticks(rotation=45)
plt.title("Number of ques-ans of each label foreach title-label")
plt.ylabel("Number of Questions & answers for each Ques-Answer label type")
plt.legend()
plt.show()

print(len(questions[0]))
print(title[1])
print(answers_text[1])
print(answers_start[1])
print(context[1])
print(len(questions))

#Now lets calculate average and median length of questions, context and answers
#For doing this we will have to consider Unique elements only
#Its easy if we convert list to set and then back to list

set_questions=set(questions)
set_context=set(context)
#set_answers=set(answers_text)

distinct_questions=list(set_questions)
distinct_context  =list(set_context  )
#distinct_answers  =list(set_answers  )# Assuming that all answers are different and if two answers match then the questions are different
def avg_length(ram):
  length=[]
  len_avg=0
  for i in range(0,len(ram)):
    if ram[i] is not None:
      len_avg+=len(ram[i])
      length.append(len(ram[i]))
    
  len_avg=len_avg/len(ram)
  return length,len_avg

answers_text_length ,   answers_text_len_avg   =   avg_length(answers_text)
distinct_context_length   ,   distinct_context_len_avg    =   avg_length(distinct_context)
distinct_questions_length   ,   distinct_questions_len_avg    =   avg_length(distinct_questions)

import statistics
print(max(answers_text_length))
print(statistics.median(distinct_context_length))
print(statistics.median(distinct_questions_length))

# print(answers_text_length ,   answers_text_len_avg)
sns.set_style("darkgrid")
plt.figure(figsize=(16, 8),facecolor="white")
sns.distplot(answers_text_length,label="answer length", rug=False, hist=True,color="black")
#sns.distplot(distinct_context_length,label="context length", rug=False, hist=True)
#sns.distplot(distinct_questions_length,label="Question length", rug=True, hist=False)
plt.xlim(0, 200)
#plt.ylim(0.005,0.05)
plt.xlabel("Length")
# plt.xticks(rotation=45)
plt.ylabel("Density function")
plt.title("Density function for length of answer")
plt.legend()
plt.show()

# print(answers_text_length ,   answers_text_len_avg)
sns.set_style("darkgrid")
plt.figure(figsize=(16, 8),facecolor="white")
#sns.distplot(answers_text_length,label="answer length", rug=True, hist=False,color="black")
sns.distplot(distinct_context_length,label="context length", rug=False, hist=True)
#sns.distplot(distinct_questions_length,label="Question length", rug=True, hist=False)
#plt.xlim(0, 200)
#plt.ylim(0.005,0.05)
plt.xlabel("Length")
# plt.xticks(rotation=45)
plt.ylabel("Density function")
plt.title("Density function for length of context")
plt.legend()
plt.show()

# print(answers_text_length ,   answers_text_len_avg)
sns.set_style("darkgrid")
plt.figure(figsize=(16,8),facecolor="white")
#sns.distplot(answers_text_length,label="answer length", rug=True, hist=False,color="black")
#sns.distplot(distinct_context_length,label="context length", rug=False, hist=True)
sns.distplot(distinct_questions_length,label="Question length", rug=False, hist=False)
plt.xlim(0,500)
#plt.ylim(0.005,0.05)
plt.xlabel("Length")
# plt.xticks(rotation=45)
plt.ylabel("Density function")
plt.title("Density function for length of questions")
plt.legend()
plt.show()